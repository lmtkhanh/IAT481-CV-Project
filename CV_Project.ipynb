{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmtkhanh/IAT481-CV-Project/blob/main/CV_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "bB55Nxr1gx8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0fmbaqswUltq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "hRQAozy5xXcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Directories (Tagging and Annotation)**"
      ],
      "metadata": {
        "id": "2QVAmyrsg3LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfMxcwGIv5tV"
      },
      "outputs": [],
      "source": [
        "\n",
        "directories = [\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "]\n",
        "\n",
        "# Loop through the directory paths and create them if they don't exist\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linking our dataset**"
      ],
      "metadata": {
        "id": "DAdqQwCQg-0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Glass images\n",
        "glass_source = '/content/drive/MyDrive/IAT 481/WasteData/Glass'\n",
        "glass_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass'\n",
        "glass_test ='/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass'\n",
        "\n",
        "#Importing Food Organics images\n",
        "organics_source = '/content/drive/MyDrive/IAT 481/WasteData/Food Organics'\n",
        "organics_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics'\n",
        "organics_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "\n",
        "#Importing Plastic images\n",
        "plastic_source = '/content/drive/MyDrive/IAT 481/WasteData/Plastic'\n",
        "plastic_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic'\n",
        "plastic_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic'\n"
      ],
      "metadata": {
        "id": "TUCWvIgvIegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Organizing dataset into according YOLO directories**"
      ],
      "metadata": {
        "id": "WMU8Oy0ChKAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def organize_file(source_dir, train_dir, test_dir):\n",
        "  files = []\n",
        "  for f in os.listdir(source_dir):\n",
        "    files.append(f)\n",
        "\n",
        "  train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "  for file in train_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(train_dir, file), \"JPEG\")\n",
        "\n",
        "  for file in test_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(test_dir, file), \"JPEG\")\n"
      ],
      "metadata": {
        "id": "zXvR95_7J76p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(organics_source, organics_train, organics_test)\n"
      ],
      "metadata": {
        "id": "tmjPcQaCXsah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(plastic_source, plastic_train, plastic_test)\n"
      ],
      "metadata": {
        "id": "jAKueMX5bqL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(glass_source, glass_train, glass_test)"
      ],
      "metadata": {
        "id": "Bv_SFj_Pcd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing and Training Model**"
      ],
      "metadata": {
        "id": "nUAzYM4hhSSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeza_8Oqf5wk",
        "outputId": "3f8724e2-af2a-49a2-b0f5-35735c387d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.8/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/IAT 481/CV-Project\""
      ],
      "metadata": {
        "id": "f4zsLTpSmuKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34b8b3f-93b5-43f7-c86d-917e636378a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IAT 481/CV-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8n-cls.pt')"
      ],
      "metadata": {
        "id": "_cQdrARofcYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d81c87-9fc6-4ace-af7e-93cb59c0d9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.30M/5.30M [00:00<00:00, 56.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=5)"
      ],
      "metadata": {
        "id": "nW_a2VsGgF5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce523fb-c8ac-4ef7-bea7-c00cd881849b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=5, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 360 images in 3 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1432 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1432/1432 [00:13<00:00, 107.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/IAT 481/CV-Project/DATA/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... 360 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:03<00:00, 111.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/IAT 481/CV-Project/DATA/test.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5         0G      1.101         16        224:   1%|          | 1/90 [00:01<02:10,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5         0G      1.135         16        224:   2%|â–         | 2/90 [00:02<01:57,  1.34s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 13.8MB/s]\n",
            "        1/5         0G      0.883          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:51<00:00,  1.23s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:18<00:00,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.869          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5         0G     0.4448          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:50<00:00,  1.23s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.878          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5         0G     0.2901          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:58<00:00,  1.31s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.922          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5         0G     0.2473          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:48<00:00,  1.20s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.95          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5         0G     0.1995          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:48<00:00,  1.20s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:14<00:00,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.175 hours.\n",
            "Optimizer stripped from runs/classify/train/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 360 images in 3 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:14<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n",
            "Speed: 0.0ms preprocess, 15.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x79226bcc3460>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9791666567325592\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9583333134651184, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9791666567325592}\n",
              "save_dir: PosixPath('runs/classify/train')\n",
              "speed: {'preprocess': 0.0010073184967041016, 'inference': 15.112641784879896, 'loss': 0.00012384520636664497, 'postprocess': 6.424056159125434e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9583333134651184\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = YOLO('yolov8n-cls.pt')"
      ],
      "metadata": {
        "id": "l58VtZcD2L1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSZjEyC2L_O",
        "outputId": "f6aff8b6-6f74-4f8c-9e7a-3923bb9314eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=8, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train2\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 360 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train2', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1432 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1432/1432 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 360 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train2\u001b[0m\n",
            "Starting training for 8 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/8         0G      0.883          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:46<00:00,  1.19s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.869          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/8         0G     0.4389          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:41<00:00,  1.13s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.878          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/8         0G     0.2842          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:41<00:00,  1.13s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.917          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/8         0G     0.2487          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:47<00:00,  1.19s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/8         0G     0.1946          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:46<00:00,  1.18s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.953          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        6/8         0G     0.1617          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:45<00:00,  1.17s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:15<00:00,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        7/8         0G     0.1334          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:45<00:00,  1.17s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        8/8         0G     0.1228          8        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [01:45<00:00,  1.17s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.967          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8 epochs completed in 0.263 hours.\n",
            "Optimizer stripped from runs/classify/train2/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train2/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 360 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.967          1\n",
            "Speed: 0.0ms preprocess, 14.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x792261ac3c70>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9833333194255829\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9666666388511658, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9833333194255829}\n",
              "save_dir: PosixPath('runs/classify/train2')\n",
              "speed: {'preprocess': 0.0008185704549153645, 'inference': 14.719809426201715, 'loss': 0.00019139713711208769, 'postprocess': 6.556510925292969e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9666666388511658\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "KxiKhReq5xMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= YOLO(\"/content/drive/MyDrive/IAT 481/CV-Project/runs/classify/train/weights/best.pt\")\n",
        "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
        "metrics.top1   # top1 accuracy\n",
        "metrics.top5   # top5 accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcQeGpi4xsL",
        "outputId": "2df57bf5-658d-4b1a-d9cd-f8bfbb20f87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 360 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 360 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:14<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n",
            "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = YOLO(\"/content/drive/MyDrive/IAT 481/CV-Project/runs/classify/train2/weights/best.pt\")\n",
        "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
        "metrics.top1   # top1 accuracy\n",
        "metrics.top5   # top5 accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA-UQnTA4zlz",
        "outputId": "a1efaf24-8d16-4a67-b993-88c74a65be4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1432 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 360 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 360 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:14<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n",
            "Speed: 0.0ms preprocess, 13.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/val3\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "s0EEfYmz51ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-glass.jpeg', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-glass-2.jpeg', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-glass-3.webp', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-plastic.jpeg', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-plastic-2.jpeg', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-plastic-3.png', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-food.jpeg', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-food-2.webp', save=True)\n",
        "results=model('/content/drive/MyDrive/IAT 481/predict-image-food-3.jpeg', save=True)\n",
        "#results\n",
        "for r in results:\n",
        "    print(r.probs, \"\\n\")"
      ],
      "metadata": {
        "id": "8TY0bABbUHrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ec99d1-641c-4464-ca65-41ecf1547860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass.jpeg: 224x224 plastic 0.90, organics 0.09, glass 0.01, 19.3ms\n",
            "Speed: 11.8ms preprocess, 19.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass-2.jpeg: 224x224 plastic 0.94, glass 0.06, organics 0.00, 16.6ms\n",
            "Speed: 29.9ms preprocess, 16.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass-3.webp: 224x224 plastic 0.85, glass 0.15, organics 0.00, 18.9ms\n",
            "Speed: 7.8ms preprocess, 18.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic.jpeg: 224x224 plastic 0.97, glass 0.03, organics 0.00, 21.0ms\n",
            "Speed: 6.1ms preprocess, 21.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic-2.jpeg: 224x224 glass 0.99, plastic 0.01, organics 0.00, 17.8ms\n",
            "Speed: 5.4ms preprocess, 17.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic-3.png: 224x224 plastic 0.78, glass 0.22, organics 0.00, 17.3ms\n",
            "Speed: 6.3ms preprocess, 17.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food.jpeg: 224x224 organics 0.97, plastic 0.03, glass 0.01, 16.5ms\n",
            "Speed: 9.4ms preprocess, 16.5ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food-2.webp: 224x224 organics 0.85, plastic 0.14, glass 0.01, 16.6ms\n",
            "Speed: 7.7ms preprocess, 16.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food-3.jpeg: 224x224 organics 0.88, glass 0.06, plastic 0.05, 17.0ms\n",
            "Speed: 6.2ms preprocess, 17.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([0.0640, 0.8842, 0.0518])\n",
            "orig_shape: None\n",
            "shape: torch.Size([3])\n",
            "top1: 1\n",
            "top1conf: tensor(0.8842)\n",
            "top5: [1, 0, 2]\n",
            "top5conf: tensor([0.8842, 0.0640, 0.0518]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-glass.jpeg', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-glass-2.jpeg', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-glass-3.webp', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-plastic.jpeg', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-plastic-2.jpeg', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-plastic-3.png', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-food.jpeg', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-food-2.webp', save=True)\n",
        "results=model2('/content/drive/MyDrive/IAT 481/predict-image-food-3.jpeg', save=True)\n",
        "#results\n",
        "for r in results:\n",
        "    print(r.probs, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ5J9uVa7aKa",
        "outputId": "a067a4d7-367c-40b0-abfb-b0ff74af8217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass.jpeg: 224x224 plastic 0.83, organics 0.11, glass 0.06, 27.3ms\n",
            "Speed: 12.9ms preprocess, 27.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass-2.jpeg: 224x224 plastic 0.99, glass 0.01, organics 0.00, 17.0ms\n",
            "Speed: 30.6ms preprocess, 17.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-glass-3.webp: 224x224 plastic 0.64, glass 0.35, organics 0.01, 24.3ms\n",
            "Speed: 7.3ms preprocess, 24.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic.jpeg: 224x224 plastic 0.96, glass 0.04, organics 0.00, 19.6ms\n",
            "Speed: 6.8ms preprocess, 19.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic-2.jpeg: 224x224 glass 1.00, plastic 0.00, organics 0.00, 22.2ms\n",
            "Speed: 4.9ms preprocess, 22.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-plastic-3.png: 224x224 plastic 0.72, glass 0.28, organics 0.00, 20.8ms\n",
            "Speed: 6.6ms preprocess, 20.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food.jpeg: 224x224 organics 0.97, plastic 0.02, glass 0.01, 19.7ms\n",
            "Speed: 7.0ms preprocess, 19.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food-2.webp: 224x224 organics 0.69, plastic 0.30, glass 0.01, 26.7ms\n",
            "Speed: 8.0ms preprocess, 26.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/IAT 481/predict-image-food-3.jpeg: 224x224 organics 0.92, plastic 0.04, glass 0.04, 19.0ms\n",
            "Speed: 6.2ms preprocess, 19.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict2\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([0.0377, 0.9238, 0.0385])\n",
            "orig_shape: None\n",
            "shape: torch.Size([3])\n",
            "top1: 1\n",
            "top1conf: tensor(0.9238)\n",
            "top5: [1, 2, 0]\n",
            "top5conf: tensor([0.9238, 0.0385, 0.0377]) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DAdqQwCQg-0u",
        "WMU8Oy0ChKAH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmtkhanh/IAT481-CV-Project/blob/main/Copy_of_CV_Project_5_10_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "bB55Nxr1gx8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/*\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GkSDwT-8Jr_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de3c99d-971c-41c9-d51e-02aebf9b55e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "hRQAozy5xXcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Directories (Tagging and Annotation)**"
      ],
      "metadata": {
        "id": "2QVAmyrsg3LL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfMxcwGIv5tV"
      },
      "outputs": [],
      "source": [
        "\n",
        "directories = [\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "]\n",
        "\n",
        "# Loop through the directory paths and create them if they don't exist\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linking our dataset**"
      ],
      "metadata": {
        "id": "DAdqQwCQg-0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "glass_source = '/content/drive/MyDrive/IAT 481/WasteData/Glass' #Change this into the path on your computer where you save our data\n",
        "glass_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass'\n",
        "glass_test ='/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass'\n",
        "\n",
        "organics_source = '/content/drive/MyDrive/IAT 481/WasteData/Food Organics' #Change this into the path on your computer\n",
        "organics_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics'\n",
        "organics_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "\n",
        "plastic_source = '/content/drive/MyDrive/IAT 481/WasteData/Plastic' #Change this into the path on your computer\n",
        "plastic_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic'\n",
        "plastic_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic'\n"
      ],
      "metadata": {
        "id": "TUCWvIgvIegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Organizing dataset into according YOLO directories**"
      ],
      "metadata": {
        "id": "WMU8Oy0ChKAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def organize_file(source_dir, train_dir, test_dir):\n",
        "  files = []\n",
        "  for f in os.listdir(source_dir):\n",
        "    files.append(f)\n",
        "\n",
        "  if len(files) == 0:\n",
        "      print(\"No files found in the source directory:\", source_dir)\n",
        "      return\n",
        "\n",
        "  train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "\n",
        "  for file in train_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(train_dir, file), \"JPEG\")\n",
        "\n",
        "  for file in test_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(test_dir, file), \"JPEG\")\n"
      ],
      "metadata": {
        "id": "zXvR95_7J76p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(organics_source, organics_train, organics_test)\n"
      ],
      "metadata": {
        "id": "tmjPcQaCXsah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(plastic_source, plastic_train, plastic_test)\n"
      ],
      "metadata": {
        "id": "jAKueMX5bqL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(glass_source, glass_train, glass_test)"
      ],
      "metadata": {
        "id": "Bv_SFj_Pcd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing and Training Model**"
      ],
      "metadata": {
        "id": "nUAzYM4hhSSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeza_8Oqf5wk",
        "outputId": "cb24d0c7-121f-4a9d-e0ec-3b9fa20ec679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.8/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PLrULjzQg4b",
        "outputId": "0f6da42c-48e6-4ec7-8a69-d6ef76c75ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "'1*jW-Q9DvmB-zvM4hgJlBx3g.png'\t\t\t  GraceKim_Week4__Neural_Networks.ipynb\n",
            " A3_GraceKim_Week_3_MachineLearning.ipynb\t 'GraceVer of CV-Project.ipynb'\n",
            " c1_1761674.jpg\t\t\t\t\t  IAT481-team20-CVproject.ipynb\n",
            "'Copy of GraceKim_Week4__Neural_Networks.ipynb'   pexels-photo-2409022.jpeg\n",
            " GraceKim_ComputerVisionProject.ipynb\t\t  runs\n",
            " GraceKim_Introduction_to_Python.ipynb\t\t  Untitled-design-17.png\n",
            "'GraceKim_Week_2_Data_Preprocessing (1).ipynb'\t  W3_GraceKim_Week_2_Data_Preprocessing.ipynb\n",
            " GraceKim_Week_2_Data_Preprocessing.ipynb\t  waste-plastic-bottle-25155974.jpg\n",
            " GraceKim_Week_3_MachineLearning.ipynb\t\t  yolov8n-cls.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8n-cls.pt')\n",
        "model2 = YOLO('yolov8n-cls.pt')"
      ],
      "metadata": {
        "id": "_cQdrARofcYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = YOLO('yolov8n-cls.pt')\n",
        "model_1.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvSZQIvpuw2R",
        "outputId": "c1d640f4-2a81-4dbf-f277-b7189ad28fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=5, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train6\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train6', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1454 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1454/1454 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 364 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train6\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5         0G     0.8732         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:56<00:00,  1.28s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.863          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5         0G     0.4434         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:47<00:00,  1.18s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.912          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5         0G     0.2778         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.945          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5         0G     0.2391         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5         0G     0.1983         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.07s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.164 hours.\n",
            "Optimizer stripped from runs/classify/train6/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train6/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train6/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n",
            "Speed: 0.0ms preprocess, 11.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train6\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train6\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7a451cd9beb0>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9780219793319702\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9560439586639404, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9780219793319702}\n",
              "save_dir: PosixPath('runs/classify/train6')\n",
              "speed: {'preprocess': 0.000992974082192222, 'inference': 11.924319214873261, 'loss': 8.252950815054086e-05, 'postprocess': 9.497443398276528e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9560439586639404\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = YOLO('yolov8n-cls.pt')\n",
        "model_2.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHG6fntCvA3R",
        "outputId": "be9de084-866c-4b8d-b386-7c31a6aa4689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=10, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train7\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train7', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1454 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1454/1454 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 364 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train7\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10         0G      0.882         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.827          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10         0G     0.4148         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.904          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10         0G      0.284         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10         0G     0.2163         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10         0G     0.1884         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.942          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10         0G     0.1574         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.959          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10         0G     0.1285         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "       8/10         0G     0.1223         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.964          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10         0G    0.09479         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.959          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "      10/10         0G    0.09814         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.953          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.310 hours.\n",
            "Optimizer stripped from runs/classify/train7/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train7/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train7/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.964          1\n",
            "Speed: 0.0ms preprocess, 12.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7a451f7db040>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9821428656578064\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9642857313156128, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9821428656578064}\n",
              "save_dir: PosixPath('runs/classify/train7')\n",
              "speed: {'preprocess': 0.0010086939885066105, 'inference': 12.405128924401252, 'loss': 9.955940665779533e-05, 'postprocess': 9.300944569346669e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9642857313156128\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "aYOZFMEN2-7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-glass.jpeg', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-glass-3.webp', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-plastic.jpeg', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-plastic-3.png', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-food.jpeg', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-food-2.webp', save=True)\n",
        "results1=model_1('/content/drive/MyDrive/predict-images/predict-image-food-3.jpeg', save=True)\n",
        "#results\n",
        "for r in results1:\n",
        "    print(r.probs, \"\\n\")"
      ],
      "metadata": {
        "id": "hQ2px3DYgsJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0713563f-4409-4ca9-d0f0-fd5b8786aa80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass.jpeg: 224x224 plastic 0.71, organics 0.25, glass 0.05, 24.1ms\n",
            "Speed: 13.4ms preprocess, 24.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train62\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg: 224x224 plastic 0.93, glass 0.07, organics 0.00, 24.7ms\n",
            "Speed: 54.5ms preprocess, 24.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train63\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-3.webp: 224x224 plastic 0.85, glass 0.13, organics 0.02, 23.4ms\n",
            "Speed: 11.6ms preprocess, 23.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train64\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic.jpeg: 224x224 plastic 1.00, glass 0.00, organics 0.00, 23.2ms\n",
            "Speed: 8.5ms preprocess, 23.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train65\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg: 224x224 glass 0.98, plastic 0.02, organics 0.00, 22.7ms\n",
            "Speed: 7.6ms preprocess, 22.7ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train66\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-3.png: 224x224 plastic 0.95, glass 0.05, organics 0.00, 23.2ms\n",
            "Speed: 9.5ms preprocess, 23.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train67\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food.jpeg: 224x224 organics 0.97, plastic 0.02, glass 0.02, 27.1ms\n",
            "Speed: 9.6ms preprocess, 27.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train68\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-2.webp: 224x224 organics 0.93, plastic 0.06, glass 0.01, 22.3ms\n",
            "Speed: 11.6ms preprocess, 22.3ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train69\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-3.jpeg: 224x224 organics 0.99, plastic 0.01, glass 0.00, 26.1ms\n",
            "Speed: 9.0ms preprocess, 26.1ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train610\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([0.0012, 0.9850, 0.0138])\n",
            "orig_shape: None\n",
            "shape: torch.Size([3])\n",
            "top1: 1\n",
            "top1conf: tensor(0.9850)\n",
            "top5: [1, 2, 0]\n",
            "top5conf: tensor([0.9850, 0.0138, 0.0012]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass-3.webp', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic-3.png', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food-2.webp', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food-3.jpeg', save=True)\n",
        "#results\n",
        "for r in results2:\n",
        "    print(r.probs, \"\\n\")"
      ],
      "metadata": {
        "id": "eXPTQxAwNjJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac997ee6-4047-43e2-ad63-5b1b0b0aadad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass.jpeg: 224x224 plastic 0.86, glass 0.09, organics 0.05, 15.4ms\n",
            "Speed: 9.1ms preprocess, 15.4ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train72\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg: 224x224 plastic 0.97, glass 0.02, organics 0.00, 18.2ms\n",
            "Speed: 30.2ms preprocess, 18.2ms inference, 0.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train73\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-3.webp: 224x224 plastic 0.61, glass 0.38, organics 0.01, 16.9ms\n",
            "Speed: 8.3ms preprocess, 16.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train74\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic.jpeg: 224x224 plastic 0.98, organics 0.01, glass 0.01, 18.0ms\n",
            "Speed: 6.1ms preprocess, 18.0ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train75\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg: 224x224 glass 0.95, plastic 0.05, organics 0.00, 16.2ms\n",
            "Speed: 5.3ms preprocess, 16.2ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train76\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-3.png: 224x224 plastic 0.89, glass 0.09, organics 0.02, 19.8ms\n",
            "Speed: 7.7ms preprocess, 19.8ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train77\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food.jpeg: 224x224 organics 1.00, glass 0.00, plastic 0.00, 16.3ms\n",
            "Speed: 6.7ms preprocess, 16.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train78\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-2.webp: 224x224 organics 0.99, plastic 0.01, glass 0.00, 16.3ms\n",
            "Speed: 8.7ms preprocess, 16.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train79\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-3.jpeg: 224x224 organics 1.00, plastic 0.00, glass 0.00, 24.3ms\n",
            "Speed: 6.2ms preprocess, 24.3ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/train710\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([1.5530e-04, 9.9916e-01, 6.8061e-04])\n",
            "orig_shape: None\n",
            "shape: torch.Size([3])\n",
            "top1: 1\n",
            "top1conf: tensor(0.9992)\n",
            "top5: [1, 2, 0]\n",
            "top5conf: tensor([9.9916e-01, 6.8061e-04, 1.5530e-04]) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DAdqQwCQg-0u",
        "WMU8Oy0ChKAH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmtkhanh/IAT481-CV-Project/blob/main/Final_CV_Project_5_10_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Libraries**"
      ],
      "metadata": {
        "id": "bB55Nxr1gx8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GkSDwT-8Jr_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01206ee3-f0cf-4de3-90a5-9adda2ae133a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n"
      ],
      "metadata": {
        "id": "hRQAozy5xXcf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Directories (Tagging and Annotation)**"
      ],
      "metadata": {
        "id": "2QVAmyrsg3LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**\n",
        "1. [RealWaste Image Classification](https://www.kaggle.com/datasets/joebeachcapital/realwaste): An existing dataset downloaded from Kaggle\n",
        " **classes:**\n",
        "  1. glass - 420 files\n",
        "  2. plastic - 921 files\n",
        "  3. organics - 411 files\n",
        "\n",
        "\n",
        "2. **Manual Data Collection**: An additional 20 files are added to each class\n",
        "\n",
        "3. **Total files**\n",
        "\n",
        "    1. glass - 440 files\n",
        "    2. plastic - 941 files\n",
        "    3. organics - 431 files\n",
        "\n",
        "\n",
        "4. **Folder Organization**\n",
        "\n",
        "\n",
        "    Root Folder\n",
        "    \n",
        "      **Train: (80% of dataset)**\n",
        "        - Food Organics\n",
        "        - Plastic\n",
        "        - Glass\n",
        "      \n",
        "    **Test: (20% of dataset)**\n",
        "        - Food Organics\n",
        "        - Plastic\n",
        "        - Glass\n"
      ],
      "metadata": {
        "id": "5bJWs-Ffc6lC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfMxcwGIv5tV"
      },
      "outputs": [],
      "source": [
        "\n",
        "directories = [\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic',\n",
        "    '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "]\n",
        "\n",
        "# Loop through the directory paths and create them if they don't exist\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linking our dataset**"
      ],
      "metadata": {
        "id": "DAdqQwCQg-0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "glass_source = '/content/drive/MyDrive/IAT 481/WasteData/Glass' #Change this into the path on your computer where you save our data\n",
        "glass_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/glass'\n",
        "glass_test ='/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/glass'\n",
        "\n",
        "organics_source = '/content/drive/MyDrive/IAT 481/WasteData/Food Organics' #Change this into the path on your computer\n",
        "organics_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/organics'\n",
        "organics_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/organics'\n",
        "\n",
        "plastic_source = '/content/drive/MyDrive/IAT 481/WasteData/Plastic' #Change this into the path on your computer\n",
        "plastic_train = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/train/plastic'\n",
        "plastic_test = '/content/drive/MyDrive/IAT 481/CV-Project/DATA/test/plastic'\n"
      ],
      "metadata": {
        "id": "TUCWvIgvIegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Organizing dataset into according YOLO directories**"
      ],
      "metadata": {
        "id": "WMU8Oy0ChKAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def organize_file(source_dir, train_dir, test_dir):\n",
        "  files = []\n",
        "  for f in os.listdir(source_dir):\n",
        "    files.append(f)\n",
        "\n",
        "  if len(files) == 0:\n",
        "      print(\"No files found in the source directory:\", source_dir)\n",
        "      return\n",
        "\n",
        "  train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
        "\n",
        "  for file in train_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(train_dir, file), \"JPEG\")\n",
        "\n",
        "  for file in test_files:\n",
        "        file_path = os.path.join(source_dir, file)\n",
        "        # Open, convert and save the image in the destination directory\n",
        "        image = Image.open(file_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image.save(os.path.join(test_dir, file), \"JPEG\")\n"
      ],
      "metadata": {
        "id": "zXvR95_7J76p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(organics_source, organics_train, organics_test)\n"
      ],
      "metadata": {
        "id": "tmjPcQaCXsah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(plastic_source, plastic_train, plastic_test)\n"
      ],
      "metadata": {
        "id": "jAKueMX5bqL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "organize_file(glass_source, glass_train, glass_test)"
      ],
      "metadata": {
        "id": "Bv_SFj_Pcd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing and Training Model**"
      ],
      "metadata": {
        "id": "nUAzYM4hhSSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeza_8Oqf5wk",
        "outputId": "cb24d0c7-121f-4a9d-e0ec-3b9fa20ec679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.8/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PLrULjzQg4b",
        "outputId": "0f6da42c-48e6-4ec7-8a69-d6ef76c75ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "'1*jW-Q9DvmB-zvM4hgJlBx3g.png'\t\t\t  GraceKim_Week4__Neural_Networks.ipynb\n",
            " A3_GraceKim_Week_3_MachineLearning.ipynb\t 'GraceVer of CV-Project.ipynb'\n",
            " c1_1761674.jpg\t\t\t\t\t  IAT481-team20-CVproject.ipynb\n",
            "'Copy of GraceKim_Week4__Neural_Networks.ipynb'   pexels-photo-2409022.jpeg\n",
            " GraceKim_ComputerVisionProject.ipynb\t\t  runs\n",
            " GraceKim_Introduction_to_Python.ipynb\t\t  Untitled-design-17.png\n",
            "'GraceKim_Week_2_Data_Preprocessing (1).ipynb'\t  W3_GraceKim_Week_2_Data_Preprocessing.ipynb\n",
            " GraceKim_Week_2_Data_Preprocessing.ipynb\t  waste-plastic-bottle-25155974.jpg\n",
            " GraceKim_Week_3_MachineLearning.ipynb\t\t  yolov8n-cls.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov8n-cls.pt')\n",
        "model2 = YOLO('yolov8n-cls.pt')"
      ],
      "metadata": {
        "id": "_cQdrARofcYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = YOLO('yolov8n-cls.pt')\n",
        "model_1.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvSZQIvpuw2R",
        "outputId": "c1d640f4-2a81-4dbf-f277-b7189ad28fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=5, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train6\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train6', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1454 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1454/1454 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 364 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train6\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/5         0G     0.8732         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:56<00:00,  1.28s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.863          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/5         0G     0.4434         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:47<00:00,  1.18s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.912          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5         0G     0.2778         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.945          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5         0G     0.2391         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5         0G     0.1983         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.07s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 epochs completed in 0.164 hours.\n",
            "Optimizer stripped from runs/classify/train6/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train6/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train6/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.956          1\n",
            "Speed: 0.0ms preprocess, 11.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train6\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train6\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7a451cd9beb0>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9780219793319702\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9560439586639404, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9780219793319702}\n",
              "save_dir: PosixPath('runs/classify/train6')\n",
              "speed: {'preprocess': 0.000992974082192222, 'inference': 11.924319214873261, 'loss': 8.252950815054086e-05, 'postprocess': 9.497443398276528e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9560439586639404\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = YOLO('yolov8n-cls.pt')\n",
        "model_2.train(data='/content/drive/MyDrive/IAT 481/CV-Project/DATA', epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHG6fntCvA3R",
        "outputId": "be9de084-866c-4b8d-b386-7c31a6aa4689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/drive/MyDrive/IAT 481/CV-Project/DATA, epochs=10, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train7\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train7', view at http://localhost:6006/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... 1454 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1454/1454 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 364 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train7\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10         0G      0.882         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.827          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10         0G     0.4148         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.904          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10         0G      0.284         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10         0G     0.2163         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       0.94          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10         0G     0.1884         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.942          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10         0G     0.1574         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.959          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10         0G     0.1285         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:39<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.951          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "       8/10         0G     0.1223         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:38<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.964          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10         0G    0.09479         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:40<00:00,  1.10s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.959          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "      10/10         0G    0.09814         14        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:37<00:00,  1.08s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.953          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.310 hours.\n",
            "Optimizer stripped from runs/classify/train7/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train7/weights/best.pt, 3.0MB\n",
            "\n",
            "Validating runs/classify/train7/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1454 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 364 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/test... found 364 images in 3 classes âœ… \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:12<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.964          1\n",
            "Speed: 0.0ms preprocess, 12.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n",
            "Results saved to \u001b[1mruns/classify/train7\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
              "\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7a451f7db040>\n",
              "curves: []\n",
              "curves_results: []\n",
              "fitness: 0.9821428656578064\n",
              "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
              "results_dict: {'metrics/accuracy_top1': 0.9642857313156128, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9821428656578064}\n",
              "save_dir: PosixPath('runs/classify/train7')\n",
              "speed: {'preprocess': 0.0010086939885066105, 'inference': 12.405128924401252, 'loss': 9.955940665779533e-05, 'postprocess': 9.300944569346669e-05}\n",
              "task: 'classify'\n",
              "top1: 0.9642857313156128\n",
              "top5: 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evalution**"
      ],
      "metadata": {
        "id": "wlfynlL-dJsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cqezf_xdKm1",
        "outputId": "ad8281a2-44e8-45e4-abb0-3f3671ad943c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CPU (AMD EPYC 7B12)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 28.9/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val: \"/content/drive/MyDrive/IAT 481/CV-Project/DATA/val\"\n",
        "\n",
        "model_2= YOLO(\"/content/drive/MyDrive/Colab Notebooks/runs/classify/train7/weights/best.pt\")  # best.pt from train 7 folder\n",
        "\n",
        "metrics = model_2.val(cfg=\"val_config.yaml\")  # no arguments needed, dataset and settings remembered\n",
        "metrics.top1   # top1 accuracy\n",
        "# not used top 5 as it will always return 100%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAvMtPFWdLyu",
        "outputId": "23561a6c-89f1-42dd-db52-eb140aacbf97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CPU (AMD EPYC 7B12)\n",
            "YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/train... found 1063 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... found 267 images in 3 classes âœ… \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/IAT 481/CV-Project/DATA/val... 267 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267/267 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:05<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.981          1\n",
            "Speed: 0.0ms preprocess, 7.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/val2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9812734127044678"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction**"
      ],
      "metadata": {
        "id": "aYOZFMEN2-7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "JKmW7BbCdOg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2= YOLO(\"/content/drive/MyDrive/Colab Notebooks/runs/classify/train7/weights/best.pt\")\n",
        "\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-glass-3.webp', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-plastic-3.png', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food.jpeg', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food-2.webp', save=True)\n",
        "results2=model_2('/content/drive/MyDrive/predict-images/predict-image-food-3.jpeg', save=True)\n",
        "#results\n",
        "for r in results2:\n",
        "    print(r.probs, \"\\n\")"
      ],
      "metadata": {
        "id": "hQ2px3DYgsJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3882ad0-3040-453e-c66c-b88e3e2e75ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass.jpeg: 224x224 plastic 0.86, glass 0.09, organics 0.05, 10.7ms\n",
            "Speed: 14.9ms preprocess, 10.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-2.jpeg: 224x224 plastic 0.97, glass 0.02, organics 0.00, 9.4ms\n",
            "Speed: 18.2ms preprocess, 9.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-glass-3.webp: 224x224 plastic 0.61, glass 0.38, organics 0.01, 9.4ms\n",
            "Speed: 4.1ms preprocess, 9.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic.jpeg: 224x224 plastic 0.98, organics 0.01, glass 0.01, 14.2ms\n",
            "Speed: 5.5ms preprocess, 14.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-2.jpeg: 224x224 glass 0.95, plastic 0.05, organics 0.00, 12.5ms\n",
            "Speed: 4.8ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-plastic-3.png: 224x224 plastic 0.89, glass 0.09, organics 0.02, 9.5ms\n",
            "Speed: 3.9ms preprocess, 9.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food.jpeg: 224x224 organics 1.00, glass 0.00, plastic 0.00, 9.4ms\n",
            "Speed: 3.7ms preprocess, 9.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-2.webp: 224x224 organics 0.99, plastic 0.01, glass 0.00, 10.2ms\n",
            "Speed: 4.5ms preprocess, 10.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/predict-images/predict-image-food-3.jpeg: 224x224 organics 1.00, plastic 0.00, glass 0.00, 10.9ms\n",
            "Speed: 3.4ms preprocess, 10.9ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "Results saved to \u001b[1mruns/classify/predict\u001b[0m\n",
            "ultralytics.engine.results.Probs object with attributes:\n",
            "\n",
            "data: tensor([1.5530e-04, 9.9916e-01, 6.8061e-04])\n",
            "orig_shape: None\n",
            "shape: torch.Size([3])\n",
            "top1: 1\n",
            "top1conf: tensor(0.9992)\n",
            "top5: [1, 2, 0]\n",
            "top5conf: tensor([9.9916e-01, 6.8061e-04, 1.5530e-04]) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}